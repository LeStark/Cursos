{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e817a0d3",
   "metadata": {},
   "source": [
    "#  IA para Redes de Suministro \n",
    "\n",
    "üë§ **Autor:** John Leonardo Vargas Mesa  \n",
    "üîó [LinkedIn](https://www.linkedin.com/in/leonardovargas/) | [GitHub](https://github.com/LeStark)  \n",
    "\n",
    "## üìÇ Repositorio en GitHub  \n",
    "- üìì **Notebooks:** [Acceder aqu√≠](https://github.com/LeStark/Cursos/tree/main/02%20-%20IA4SC)  \n",
    "- üìë **Data sets:** [Acceder aqu√≠](https://github.com/LeStark/Cursos/tree/main/00%20-%20Data/02%20-%20SC)  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3fac3",
   "metadata": {},
   "source": [
    "# üìò Notebook 5 ‚Äì Introducci√≥n a Natural Lenguage Processing para redes de suministro\n",
    "\n",
    "Este notebook introduce los conceptos fundamentales del **Procesamiento de Lenguaje Natural (NLP)** a trav√©s de un caso pr√°ctico de **an√°lisis de sentimientos** utilizando rese√±as de productos de Amazon.\n",
    "\n",
    "A lo largo del notebook se recorren las principales etapas del flujo de trabajo en NLP, desde la limpieza del texto hasta la interpretaci√≥n de los resultados del modelo.\n",
    "\n",
    "## üß© **Contenido del Notebook**\n",
    "\n",
    "1. **Carga y Exploraci√≥n del Dataset:**  \n",
    "   Se utiliza un conjunto de rese√±as reales de Amazon que incluye texto y calificaci√≥n (`overall`). Se realiza una exploraci√≥n inicial para comprender la estructura de los datos.\n",
    "\n",
    "2. **Conversi√≥n de Calificaciones a Sentimientos:**  \n",
    "   Se crea una columna categ√≥rica (`sentiment`) basada en la calificaci√≥n:  \n",
    "   - `Positive` (‚â• 4)  \n",
    "   - `Neutral` (= 3)  \n",
    "   - `Negative` (‚â§ 2)\n",
    "\n",
    "3. **Codificaci√≥n y An√°lisis de Balance:**  \n",
    "   Se codifican las etiquetas num√©ricamente (`LabelEncoder`) y se visualiza la distribuci√≥n de clases para detectar posibles desbalances en el dataset.\n",
    "\n",
    "4. **Preprocesamiento del Texto:**  \n",
    "   Se aplican t√©cnicas de limpieza y normalizaci√≥n como:\n",
    "   - Conversi√≥n a min√∫sculas  \n",
    "   - Eliminaci√≥n de signos, n√∫meros y URLs  \n",
    "   - Tokenizaci√≥n  \n",
    "   - Eliminaci√≥n de *stopwords*  \n",
    "   - Lematizaci√≥n con spaCy  \n",
    "\n",
    "   El resultado se almacena en la columna `clean_text`.\n",
    "\n",
    "5. **Balanceo del Dataset:**  \n",
    "   Se realiza un **submuestreo** para igualar la cantidad de rese√±as por clase, creando un conjunto balanceado llamado `amazon_reviews_balanced`.\n",
    "\n",
    "6. **Vectorizaci√≥n (TF-IDF):**  \n",
    "   Se transforma el texto limpio en vectores num√©ricos mediante **TF-IDF**, limitando el vocabulario a las palabras m√°s relevantes.\n",
    "\n",
    "7. **Entrenamiento del Modelo:**  \n",
    "   Se entrena una **Regresi√≥n Log√≠stica** con ajuste de pesos (`class_weight='balanced'`) para manejar el desbalance de clases.\n",
    "\n",
    "8. **Evaluaci√≥n del Modelo:**  \n",
    "   Se generan m√©tricas de rendimiento (precision, recall, f1-score) y se visualiza la matriz de confusi√≥n para interpretar los resultados.\n",
    "\n",
    "9. **Predicci√≥n sobre Nuevos Textos:**  \n",
    "   Se define una funci√≥n `predict_sentiment()` que integra todo el pipeline (preprocesamiento + vectorizaci√≥n + modelo + decodificaci√≥n) para clasificar nuevas rese√±as.\n",
    "\n",
    "10. **Prueba con Datos Nuevos:**  \n",
    "    Se aplica la funci√≥n a un archivo de rese√±as de muestra (`sample_amazon_reviews.csv`) y se observan las predicciones de sentimiento.\n",
    "\n",
    "## üéØ **Objetivo de Aprendizaje**\n",
    "\n",
    "Al finalizar el notebook, el estudiante podr√°:\n",
    "- Comprender el flujo completo de un proyecto de NLP.  \n",
    "- Implementar un pipeline b√°sico de an√°lisis de sentimientos.  \n",
    "- Aplicar t√©cnicas de preprocesamiento y vectorizaci√≥n de texto.  \n",
    "- Entrenar, evaluar y reutilizar un modelo de Machine Learning aplicado a lenguaje natural.\n",
    "\n",
    "## üßæ **Herramientas Utilizadas**\n",
    "- **Python:** procesamiento y modelado  \n",
    "- **NLTK / spaCy:** limpieza, tokenizaci√≥n y lematizaci√≥n  \n",
    "- **scikit-learn:** vectorizaci√≥n, modelado y evaluaci√≥n  \n",
    "- **Matplotlib / Seaborn:** visualizaci√≥n de resultados  \n",
    "\n",
    "*Este notebook sirve como punto de partida para comprender los fundamentos pr√°cticos del procesamiento de lenguaje natural y la construcci√≥n de modelos de an√°lisis de texto.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f69b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala las bibliotecas necesarias si no las tienes instaladas\n",
    "!pip install pandas scikit-learn nltk spacy matplotlib seaborn imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Manipulaci√≥n y an√°lisis de datos ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Procesamiento de texto ---\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Modelado y vectorizaci√≥n ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --- Manejo de desbalance ---\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# --- Visualizaci√≥n ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Descarga de recursos NLTK ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# --- Carga del modelo de lenguaje de spaCy ---\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2ca10",
   "metadata": {},
   "source": [
    "**Carga y Exploraci√≥n Inicial del Dataset**\n",
    "\n",
    "En esta secci√≥n cargamos el dataset **Amazon Reviews**, que contiene opiniones reales de usuarios sobre distintos productos publicados en la plataforma de Amazon.  \n",
    "El archivo se encuentra alojado en un repositorio p√∫blico de GitHub y se importa directamente desde su URL en formato CSV.\n",
    "\n",
    "###  **Descripci√≥n del Dataset**\n",
    "\n",
    "El dataset **Amazon Reviews** contiene rese√±as de productos realizadas por usuarios en la plataforma de Amazon.  \n",
    "Incluye tanto el texto de la rese√±a como m√©tricas de utilidad y calificaciones otorgadas por otros usuarios.\n",
    "\n",
    "A continuaci√≥n se describen las columnas principales:\n",
    "\n",
    "| **Columna** | **Descripci√≥n** |\n",
    "|--------------|-----------------|\n",
    "| `reviewerName` | Nombre del usuario que escribi√≥ la rese√±a. |\n",
    "| `overall` | Calificaci√≥n general otorgada al producto, en una escala de **1 a 5** (1 = muy mala, 5 = excelente). |\n",
    "| `reviewText` | Texto libre con la opini√≥n o experiencia del usuario respecto al producto. |\n",
    "| `reviewTime` | Fecha en la que la rese√±a fue publicada. |\n",
    "| `day_diff` | Diferencia en d√≠as entre la fecha de la rese√±a y una fecha de referencia (√∫til para an√°lisis temporales). |\n",
    "| `helpful_yes` | N√∫mero de votos que consideraron la rese√±a como √∫til. |\n",
    "| `helpful_no` | N√∫mero de votos que consideraron la rese√±a como **no √∫til**. |\n",
    "| `total_vote` | Total de votos recibidos (`helpful_yes + helpful_no`). |\n",
    "| `score_pos_neg_diff` | Diferencia entre votos positivos y negativos (mide la percepci√≥n general de utilidad). |\n",
    "| `score_average_rating` | Promedio de la puntuaci√≥n basada en votos de utilidad. |\n",
    "| `wilson_lower_bound` | Estimaci√≥n estad√≠stica de la utilidad de la rese√±a considerando la incertidumbre de los votos (basada en el m√©todo de Wilson). |\n",
    "\n",
    "Este conjunto de datos es ideal para realizar **an√°lisis de sentimientos** y explorar la relaci√≥n entre las calificaciones num√©ricas (`overall`) y las opiniones escritas (`reviewText`), complementadas con m√©tricas de confianza y popularidad de las rese√±as.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: cargar dataset \"https://raw.githubusercontent.com/LeStark/Cursos/refs/heads/main/00%20-%20Data/03%20-%20NLP/amazon_reviews.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df1e38",
   "metadata": {},
   "source": [
    "### **Selecci√≥n de Columnas Relevantes**\n",
    "\n",
    "En esta etapa se seleccionan √∫nicamente las columnas necesarias para el an√°lisis de sentimientos:  \n",
    "- `reviewText`: texto de la rese√±a.  \n",
    "- `overall`: calificaci√≥n num√©rica otorgada por el usuario.  \n",
    "\n",
    "Esto simplifica el dataset para enfocarnos en las variables clave del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: seleccionar solo las columnas de interes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ddf41",
   "metadata": {},
   "source": [
    "### **Conversi√≥n de Calificaciones Num√©ricas a Categor√≠as de Sentimiento**\n",
    "\n",
    "En esta secci√≥n se transforma la columna `overall` (puntuaci√≥n de 1 a 5) en una variable categ√≥rica llamada `sentiment`.  \n",
    "Esta nueva columna clasifica cada rese√±a seg√∫n el tono general de la opini√≥n del usuario:\n",
    "\n",
    "- **Positive (‚â• 4):** opiniones favorables o muy satisfechas.  \n",
    "- **Neutral (= 3):** opiniones intermedias o sin una posici√≥n clara.  \n",
    "- **Negative (‚â§ 2):** opiniones desfavorables o con experiencias negativas.  \n",
    "\n",
    "Esta conversi√≥n facilita el entrenamiento de modelos de **an√°lisis de sentimientos**, al convertir las valoraciones num√©ricas en etiquetas textuales comprensibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conversi√≥n de 'overall' a etiquetas de sentimiento ---\n",
    "#TODO: definir una funci√≥n para convertir 'overall' a 'positive', 'neutral', 'negative'\n",
    "\n",
    "# Crear la nueva columna\n",
    "amazon_reviews[\"sentiment\"] = amazon_reviews[\"overall\"].apply(get_sentiment)\n",
    "\n",
    "# Mostrar una muestra\n",
    "amazon_reviews.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982edf44",
   "metadata": {},
   "source": [
    "### **Codificaci√≥n de las Etiquetas de Sentimiento**\n",
    "\n",
    "En esta etapa se convierte la columna `sentiment`, que contiene valores categ√≥ricos (*positive*, *neutral*, *negative*), en valores num√©ricos mediante la clase `LabelEncoder` de **scikit-learn**.  \n",
    "\n",
    "Esto es necesario porque los algoritmos de Machine Learning trabajan con variables num√©ricas.  \n",
    "Cada etiqueta de texto se transforma en un n√∫mero entero √∫nico, por ejemplo:\n",
    "\n",
    "- **negative ‚Üí 0**  \n",
    "- **neutral ‚Üí 1**  \n",
    "- **positive ‚Üí 2**\n",
    "\n",
    "El resultado se almacena en una nueva columna llamada `sentiment_encoded`, que servir√° como variable objetivo (`y`) durante el entrenamiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dc3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Codificar la columna 'sentiment' ---\n",
    "le = #usar el codiicador de etiquetas de sklearn\n",
    "\n",
    "# Ajustar el codificador y transformar la columna\n",
    "\n",
    "#TODO: definir la nueva columna con los valores codificados llamada sentiment_encoded\n",
    "\n",
    "# Mostrar c√≥mo se asignaron los valores\n",
    "for clase, codigo in zip(le.classes_, range(len(le.classes_))):\n",
    "    print(f\"{clase}: {codigo}\")\n",
    "\n",
    "# Vista r√°pida del DataFrame\n",
    "amazon_reviews.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f757d",
   "metadata": {},
   "source": [
    "###  **Distribuci√≥n de Sentimientos y An√°lisis de Balance de Clases**\n",
    "\n",
    "En esta secci√≥n se visualiza la cantidad de rese√±as por categor√≠a de sentimiento (*positive*, *neutral*, *negative*).  \n",
    "El gr√°fico de barras permite identificar si existe **desbalance de clases**, es decir, si alguna categor√≠a tiene una cantidad de muestras mucho mayor que las dem√°s.\n",
    "\n",
    "Este an√°lisis es fundamental antes de entrenar el modelo, ya que un conjunto de datos desequilibrado puede generar sesgos en las predicciones.  \n",
    "Por ejemplo, si la mayor√≠a de rese√±as son positivas, el modelo tender√° a clasificar casi todo como *positive*, afectando la precisi√≥n en las clases minoritarias.\n",
    "\n",
    "La informaci√≥n obtenida aqu√≠ servir√° para decidir estrategias de balanceo como:\n",
    "- **Submuestreo o sobremuestreo** de clases.  \n",
    "- **Ajuste de pesos de clase** durante el entrenamiento.  \n",
    "- **Evaluaci√≥n con m√©tricas balanceadas** (precision, recall, F1-score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Contar cu√°ntas rese√±as hay por sentimiento\n",
    "sentiment_counts = amazon_reviews[\"sentiment\"].value_counts().sort_index()\n",
    "\n",
    "# Crear gr√°fico de barras\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=[\"red\", \"gray\", \"green\"])\n",
    "\n",
    "# Etiquetas y t√≠tulo\n",
    "plt.title(\"Distribuci√≥n de Sentimientos en Amazon Reviews\", fontsize=14)\n",
    "plt.xlabel(\"Sentimiento\", fontsize=12)\n",
    "plt.ylabel(\"N√∫mero de Rese√±as\", fontsize=12)\n",
    "\n",
    "# Mostrar los valores encima de cada barra\n",
    "for i, value in enumerate(sentiment_counts.values):\n",
    "    plt.text(i, value + (value*0.01), str(value), ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259dcde9",
   "metadata": {},
   "source": [
    "### **Preprocesamiento del Texto**\n",
    "\n",
    "El preprocesamiento es una etapa esencial en cualquier proyecto de **Procesamiento de Lenguaje Natural (NLP)**, ya que prepara los textos para que los algoritmos puedan interpretarlos correctamente.  \n",
    "En esta celda se aplican diferentes t√©cnicas de limpieza y normalizaci√≥n al texto de las rese√±as.\n",
    "\n",
    "#### **Pasos realizados:**\n",
    "\n",
    "1. **Descarga de recursos ling√º√≠sticos:**  \n",
    "   Se descargan los diccionarios de *stopwords* (palabras vac√≠as como ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúis‚Äù) y el tokenizador de `nltk`.  \n",
    "   Adem√°s, se carga el modelo de lenguaje de **spaCy** (`en_core_web_sm`) para poder realizar la lematizaci√≥n.\n",
    "\n",
    "2. **Conversi√≥n a min√∫sculas:**  \n",
    "   Unifica el texto y evita que palabras como *‚ÄúGood‚Äù* y *‚Äúgood‚Äù* se traten como diferentes.\n",
    "\n",
    "3. **Eliminaci√≥n de ruido:**  \n",
    "   Se remueven URLs, signos de puntuaci√≥n, n√∫meros y otros caracteres que no aportan significado.\n",
    "\n",
    "4. **Tokenizaci√≥n:**  \n",
    "   El texto se divide en palabras individuales (*tokens*) para su an√°lisis.\n",
    "\n",
    "5. **Eliminaci√≥n de stopwords:**  \n",
    "   Se eliminan palabras comunes sin valor sem√°ntico relevante para el an√°lisis (por ejemplo: *‚Äúthis‚Äù, ‚Äúthat‚Äù, ‚Äúwas‚Äù*).\n",
    "\n",
    "6. **Lematizaci√≥n:**  \n",
    "   Cada palabra se transforma en su forma base o ra√≠z (*‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù*), lo que ayuda a reducir la dimensionalidad del texto.\n",
    "\n",
    "7. **Reconstrucci√≥n del texto limpio:**  \n",
    "   Finalmente, las palabras procesadas se unen nuevamente en una cadena, generando una nueva columna llamada `clean_text`.\n",
    "\n",
    "Esta nueva versi√≥n del texto ser√° la base para la **vectorizaci√≥n** y el **entrenamiento del modelo de an√°lisis de sentimientos**, garantizando que los datos est√©n homog√©neos y sin ruido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPROCESAMIENTO DE TEXTO PARA NLP ---\n",
    "\n",
    "# En esta secci√≥n se realiza la limpieza y normalizaci√≥n del texto.\n",
    "# Este paso es fundamental antes de entrenar un modelo de NLP, \n",
    "# ya que los algoritmos no pueden trabajar directamente con texto sin estructurar.\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "#  1. Descarga de recursos necesarios (solo la primera vez)\n",
    "# -------------------------------------------------------------\n",
    "# 'stopwords': lista de palabras vac√≠as como \"the\", \"and\", \"is\"\n",
    "# 'punkt': modelo de tokenizaci√≥n para dividir el texto en palabras\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Cargar el modelo de lenguaje de spaCy (usado para la lematizaci√≥n)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. Definici√≥n de palabras vac√≠as (stopwords)\n",
    "# -------------------------------------------------------------\n",
    "# Estas palabras no aportan informaci√≥n relevante al an√°lisis de sentimiento,\n",
    "# por lo que se eliminan del texto.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Funci√≥n de preprocesamiento\n",
    "# -------------------------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Esta funci√≥n limpia y normaliza un texto en varios pasos:\n",
    "    1. Convierte el texto a min√∫sculas.\n",
    "    2. Elimina URLs, caracteres especiales, n√∫meros y signos.\n",
    "    3. Tokeniza el texto (divide en palabras).\n",
    "    4. Elimina stopwords y palabras muy cortas.\n",
    "    5. Aplica lematizaci√≥n con spaCy (reduce palabras a su ra√≠z).\n",
    "    6. Reconstruye el texto limpio.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1Ô∏è Pasar a min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2Ô∏è Eliminar URLs, menciones, signos y n√∫meros\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)                 # Solo letras\n",
    "    \n",
    "    # 3Ô∏è Tokenizaci√≥n (dividir en palabras)\n",
    "    #TODO\n",
    "    \n",
    "    # 4Ô∏è Eliminar stopwords y palabras de longitud < 3\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # 5Ô∏è Lematizaci√≥n con spaCy\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # 6Ô∏è Reconstruir el texto limpio\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. Aplicar el preprocesamiento al dataset\n",
    "# -------------------------------------------------------------\n",
    "# Convertimos la columna 'reviewText' en texto limpio y normalizado.\n",
    "# Este proceso puede tardar algunos minutos si el dataset es grande.\n",
    "#TODO\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5. Visualizar los resultados\n",
    "# -------------------------------------------------------------\n",
    "# Mostramos una muestra comparando el texto original con el texto procesado.\n",
    "amazon_reviews[[\"reviewText\", \"clean_text\"]].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1f087",
   "metadata": {},
   "source": [
    "### **Balanceo del Dataset mediante Submuestreo**\n",
    "\n",
    "En esta etapa se busca **equilibrar la cantidad de rese√±as por cada categor√≠a de sentimiento** para evitar que el modelo se sesgue hacia la clase mayoritaria (por lo general, *positive*).  \n",
    "\n",
    "#### **Procedimiento:**\n",
    "1. Se identifica el n√∫mero m√≠nimo de muestras entre las clases (en este caso, la cantidad de rese√±as *negative*).  \n",
    "2. Se selecciona al azar la misma cantidad de rese√±as *positive* y *neutral* para igualar el tama√±o de cada grupo.  \n",
    "   - En el caso de las rese√±as *neutral*, se permite el muestreo con reemplazo si son muy pocas.  \n",
    "3. Se combinan los tres subconjuntos y se reordenan aleatoriamente para crear un nuevo DataFrame llamado `amazon_reviews_balanced`.  \n",
    "\n",
    "#### **Resultado:**\n",
    "El nuevo dataset tiene la misma cantidad de ejemplos por clase (*positive*, *neutral*, *negative*), lo cual facilita que el modelo aprenda de forma equitativa y reduzca el sesgo hacia una categor√≠a dominante.\n",
    "\n",
    "La gr√°fica resultante muestra visualmente la distribuci√≥n balanceada de los sentimientos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad m√≠nima entre clases (usaremos la de 'negative')\n",
    "min_count = amazon_reviews[\"sentiment\"].value_counts()[\"negative\"]\n",
    "\n",
    "# Separar por clase\n",
    "df_pos = amazon_reviews[amazon_reviews[\"sentiment\"] == \"positive\"].sample(n=min_count, random_state=42)\n",
    "df_neu = amazon_reviews[amazon_reviews[\"sentiment\"] == \"neutral\"].sample(n=min_count, random_state=42, replace=True)  # opcional: con reemplazo si hay pocas\n",
    "df_neg = amazon_reviews[amazon_reviews[\"sentiment\"] == \"negative\"]\n",
    "\n",
    "# Unir los subconjuntos balanceados\n",
    "amazon_reviews_balanced = pd.concat([df_pos, df_neu, df_neg], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verificar el nuevo balance\n",
    "print(\"üîπ Distribuci√≥n balanceada:\")\n",
    "print(amazon_reviews_balanced[\"sentiment\"].value_counts())\n",
    "\n",
    "# (Opcional) Visualizar\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=\"sentiment\", data=amazon_reviews_balanced, palette=[\"red\", \"gray\", \"green\"])\n",
    "plt.title(\"Dataset Balanceado (Submuestreo)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7daf1a",
   "metadata": {},
   "source": [
    "### **Vectorizaci√≥n del Texto con TF-IDF**\n",
    "\n",
    "En esta secci√≥n se transforma el texto limpio (`clean_text`) en una representaci√≥n num√©rica que los modelos de Machine Learning puedan interpretar.  \n",
    "Para ello se utiliza la t√©cnica **TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)**, que mide la importancia de cada palabra en relaci√≥n con el conjunto de documentos.\n",
    "\n",
    "#### **Proceso:**\n",
    "1. Se crea un objeto `TfidfVectorizer` que convierte las palabras en vectores num√©ricos.  \n",
    "   - El par√°metro `max_features=1000` limita el vocabulario a las 1000 palabras m√°s relevantes, evitando un modelo demasiado grande.  \n",
    "2. Se aplica el m√©todo `fit_transform()` sobre la columna `clean_text` para generar la matriz `X`, donde:\n",
    "   - Cada fila representa una rese√±a.  \n",
    "   - Cada columna representa una palabra del vocabulario.  \n",
    "   - Los valores reflejan el peso TF-IDF de cada palabra.  \n",
    "3. Finalmente, la variable `y` almacena las etiquetas num√©ricas (`sentiment_encoded`) que corresponden al sentimiento de cada rese√±a.\n",
    "\n",
    "El resultado es una estructura matricial donde los textos quedan representados de forma cuantitativa, lista para alimentar al modelo de clasificaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc798d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear el vectorizador\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # puedes ajustar el n√∫mero de caracter√≠sticas\n",
    "\n",
    "# Ajustar y transformar el texto limpio\n",
    "X = vectorizer.fit_transform(amazon_reviews_balanced[\"clean_text\"])\n",
    "\n",
    "# Variable objetivo\n",
    "y = amazon_reviews_balanced[\"sentiment_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57203b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Elegir un √≠ndice aleatorio o fijo para mostrar\n",
    "idx = np.random.randint(0, X.shape[0])\n",
    "\n",
    "# Texto original y limpio\n",
    "print(\"üîπ Review original:\")\n",
    "print(amazon_reviews.loc[idx, \"reviewText\"])\n",
    "print(\"\\nüîπ Texto preprocesado:\")\n",
    "print(amazon_reviews.loc[idx, \"clean_text\"])\n",
    "print(\"\\nüîπ Sentimiento:\", amazon_reviews.loc[idx, \"sentiment\"])\n",
    "\n",
    "# Obtener las palabras del vocabulario\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convertir el vector a un DataFrame legible\n",
    "vector_df = pd.DataFrame(\n",
    "    X[idx].toarray().flatten(),\n",
    "    index=feature_names,\n",
    "    columns=[\"TF-IDF Value\"]\n",
    ")\n",
    "\n",
    "# Mostrar solo las palabras con peso no cero\n",
    "vector_df = vector_df[vector_df[\"TF-IDF Value\"] > 0].sort_values(by=\"TF-IDF Value\", ascending=False)\n",
    "\n",
    "print(\"\\nüîπ Palabras m√°s relevantes en este review:\")\n",
    "display(vector_df.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84cf2c",
   "metadata": {},
   "source": [
    "### **Divisi√≥n del Conjunto de Datos**\n",
    "\n",
    "Se divide el dataset en dos subconjuntos:  \n",
    "- **80%** para entrenamiento (`X_train`, `y_train`)  \n",
    "- **20%** para prueba (`X_test`, `y_test`)  \n",
    "\n",
    "El par√°metro `stratify=y` garantiza que la proporci√≥n de clases se mantenga equilibrada en ambos conjuntos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c3ea1",
   "metadata": {},
   "source": [
    "### **Entrenamiento del Modelo de Clasificaci√≥n**\n",
    "\n",
    "Se entrena un modelo de **Regresi√≥n Log√≠stica** para predecir el sentimiento de las rese√±as.  \n",
    "El par√°metro `class_weight='balanced'` ajusta autom√°ticamente el peso de cada clase para compensar posibles desbalances en los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48186dda",
   "metadata": {},
   "source": [
    "### **Evaluaci√≥n del Modelo**\n",
    "\n",
    "Se generan las m√©tricas de desempe√±o del modelo mediante `classification_report`, que muestra **precisi√≥n**, **recobrado (recall)** y **F1-score** por clase.  \n",
    "Adem√°s, la **matriz de confusi√≥n** permite visualizar los aciertos y errores en las predicciones de cada categor√≠a de sentimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f812507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reporte de m√©tricas\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=le.classes_)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249be25a",
   "metadata": {},
   "source": [
    "### **Prueba del Modelo con Nuevas Rese√±as**\n",
    "\n",
    "En esta secci√≥n se prueban ejemplos de texto reales para evaluar el comportamiento del modelo entrenado.  \n",
    "Cada rese√±a se limpia con la misma funci√≥n de preprocesamiento y luego se transforma en un vector TF-IDF antes de ser clasificada.\n",
    "\n",
    "El modelo devuelve la categor√≠a de sentimiento predicha (**positive**, **neutral** o **negative**), lo que permite comprobar de manera pr√°ctica c√≥mo interpreta nuevas opiniones de usuarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rese√±a_1 = \"I bought 2 of those SanDisk 32 GB microSD , used them on my Galaxy Note and Galaxy S4First one , my phone started saying it was removed , then recognize it again :) then diedI thought it's just a luck , plugged in the 2nd one :) stayed for about 2 months and died suddenly ! and lost everythingnever buying from SanDisk again .. ever\"\n",
    "rese√±a_2 = \"This product is amazing, I totally recommend it!\"\n",
    "nuevo_texto = [rese√±a_2]\n",
    "nuevo_texto_limpio = [preprocess_text(t) for t in nuevo_texto]\n",
    "nuevo_vector = vectorizer.transform(nuevo_texto_limpio)\n",
    "pred = model.predict(nuevo_vector)\n",
    "print(\"Predicci√≥n:\", le.inverse_transform(pred)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05376eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"sentiment_model.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(le, \"label_encoder.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
